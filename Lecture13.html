<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
  <link rel="stylesheet" href="style.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        renderActions: {
          addMenu: []
        }
      },
      svg: {
        fontCache: 'global',
        scale: 1.0
      }
    };
    document.addEventListener("DOMContentLoaded", () => {
    const sidebar = document.querySelector('.sidebar');
    const main = document.querySelector('.main-content');
    const wrapper = document.createElement('div');
    wrapper.className = 'layout';

    sidebar.parentNode.insertBefore(wrapper, sidebar);
    wrapper.appendChild(sidebar);
    wrapper.appendChild(main);
  });
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>

<body>
  <div class="sidebar">
    <img src="Logo_NS-blackboard-Expand.png" alt="Sidebar Image">
    <nav>
      <ul class="nav-menu">
        <li><a href="Welcome.html">Welcome to NFYK22003U</a></li>
           <ul class="sub-menu">
            <li><a href="CourseInformation.html">Course Information</a></li>
            <li><a href="Schedule2025.html">Schedule 2025</a></li>
          </ul>
        <li><a href="MathsSpeedrun.html">Maths Speedrun</a></li>
        <li>
          <a href="Lecture1.html">Lecture 1</a></li>
          <ul class="sub-menu">
            <li><a href="Project1.html">Project 1 – Oxygen Budget</a></li>
          </ul>
          <li><a href="Lecture2.html">Lecture 2</a></li>
          <ul class="sub-menu">
            <li><a href="Project2.html">Project 2 – The Sverdrup Relation</a></li>
          </ul>
          <li><a href="Lecture3.html">Lecture 3</a></li>
          <li><a href="Lecture4.html">Lecture 4</a></li>
          <ul class="sub-menu">
            <li><a href="Project3.html">Project 3 – Western Boundary Currents</a></li>
          </ul>
          <li><a href="Lecture5.html">Lecture 5</a></li>
          <li><a href="Lecture6.html">Lecture 6</a></li>
          <li><a href="Lecture7.html">Lecture 7</a></li>
          <ul class="sub-menu">
            <li><a href="Project4.html">Project 4 – Shadow Zone</a></li>
          </ul>
          <li><a href="Lecture8.html">Lecture 8</a></li>
          <li><a href="Lecture9.html">Lecture 9</a></li>
          <ul class="sub-menu">
            <li><a href="Project5.html">Project 5 – Hadley Cell</a></li>
          </ul>
          <li><a href="Lecture10.html">Lecture 10</a></li>
          <ul class="sub-menu">
            <li><a href="Project6.html">Project 6 – Southern Ocean Winds and Overturning</a></li>
          </ul>
          <li><a href="Lecture11.html">Lecture 11</a></li>
          <li><a href="Lecture12.html">Lecture 12</a></li>
          <ul class="sub-menu">
            <li><a href="Project7.html">Project 7 – Oscillations</a></li>
            <li><a href="TestExam.html">Test Exam</a></li>
          </ul>
          <li><a href="Lecture13.html">Lecture 13</a></li>
          <ul class="sub-menu">
            <li><a href="SailbyNightPhysics.html">Sail by Night Physics</a></li>
          </ul>
          <li><a href="References.html">References</a></li>
        </li>
      </ul>
    </nav>
  </div>

  <div class="main-content">
    <h1>Lecture 13 – Applications of Machine Learning in CFD and GFD</h1>

    <img src="Figures/Lecture13_NBIBuildingMLBanner.png" alt="NBIBuildingMLBanner" style="margin-top: 1.5em; border-radius: 6px; max-width: 100%; display: block;">     

    <h2>What Is Learning</h2>
    Falsifiability is fundamental to learning. 
    If a theory is falsifiable then it is learnable – i.e. admits a
strategy that predicts optimally. An analogous result is shown for universal induction.
<div style="text-align: center;">
  <i>A theory that explains everything, [predicts] nothing.</i>
</div>

Reference: <a href="https://philarchive.org/archive/BALFIL">Falsifiable ⟹ Learnable</a>

    
    <h2>What Is Machine Learning</h2>
    The goal of machine learning is to make computers “learn” from “data”. From an end user’s perspective, it is about understanding your data, make
predictions and decisions. Intellectually, it is a collection of models, methods and algorithms that have evolved over
more than a half-century now.

    <h2>Machine Learning vs Statistics</h2>
    Historically both disciplines evolved from different perspectives, but with similar end goals. 
    For example, Machine Learning focused on “prediction” and “decisions”. 
    It relied on “patterns” or “model” learnt in the process to achieve it. 
    Computation has played key role in its evolution. 
    In contrast, Statistics, founded by statisticians such as Pearson and Fisher, 
    focused on “model learning”. 
    To understand and explain “why” behind a phenomenon. 
    Probability has played key role in development of the field. <br>
    
    As a concrete example, recall the ideal gas law $PV = nRT$ for Physics. 
    Historically, machine learning only cared about ability to predict $P$ by knowing $V$ and $T$, did not matter how; 
    on the other hand, Statistics did care about the precise form of the relationship between $P, V$ and $T$, in particular it being linear. 
    Having said that, in current day and age, both disciplines are getting closer and closer, day-by-day.

    <h2>Machine Learning vs Artificial Intelligence</h2>
    Artificial Intelligence’s stated goal is to <span class="doubleUnderline">mimic human behavior in an
intelligent manner</span>, and to do what humans can do but really well, which includes artificial “creativity” and driving
cars, playing games, responding to consumer questions, etc. Traditionally, the main tools to achieve these goals are
“rules” and “decision trees”. 
In that sense, Artificial intelligence 
seeks to create <i>muscle</i> and <i>mind</i> of humans, and
<i>mind</i> requires learning from data, i.e. Machine Learning. However, Machine Learning helps learn from data beyond
mimicking humans. Having said that, again the boundaries between AI and ML are getting blurry, day-by-day.


    <h2>Neural Networks and Deep Learning</h2>
Neural networks (NNs) were inspired by the Nobel Prize winning work of Hubel and Wiesel on the how visual processing of cats starts with simple structure( ex. oriented edges) and brain builds up the complexity of the visual information
until it recognizes the complex visual world. Their experiments showed that neuronal networks were organized in hierarchical layers of cells for processing visual stimuli.
  <img src="Figures/Lecture13_NobelCat.png" alt="NobelCat" style="margin-top: 1.5em; border-radius: 6px; max-width: 100%; display: block;">     
    <div class="figure-caption">(An experiment where a cat sees different images or stimuli.
      Credit: Ho Jin Lee.)</div>    
      The recent success of NNs has been enabled by two critical components:
      <ul>
        <li>the continued growth of computational power</li>
        <li>exceptionally large labeled datasets</li>
      </ul> 
       NNs specifically optimize over a compositional function
$$
\overbrace{
\underbrace{\arg\min_{A_j}}_{\text{optimization over network weights}} 
\left[
f_M(A_M, \dots, f_2(A_2, f_1(A_1, x)) \dots ) 
+ 
\underbrace{\lambda g(A_j)}_{\text{regularization term}}
\right]
}^{\text{overall neural network training objective}}
$$

For deep learning models, instead of optimizing individual weight matrices $A_1, A_2, \dots, A_M$ at each layer, deep learning frameworks bundle all parameters into a single vector or set $\theta$:

$$
\arg\min_\theta f_\theta(x)
$$

where $\theta$ are the neural network weights and $f(\cdot)$ characterizes the network (number of layers, structure, regularizers).<br>

It can be seen that the learning process in deep learning becomes a problem of finding the parameters $\theta$ that minimize a loss function over data:

  $$
  \arg\min_\theta \mathbb{E}_{x \sim \mathcal{D}} \left[ \mathcal{L}(f_\theta(x), y) \right]
  $$
 

    <h2>Machine Learning in Computational Fluid Dynamics</h2>

    <h2>Machine Learning in Geophysical Fluid Dynamics</h2>

    <h2>Data-Driven Dynamical Systems</h2>

    But more is different.

      <img src="Figures/Lecture13_SnowFlakesFractal.png" alt="SnowFlakesFractal" style="margin-top: 1.5em; border-radius: 6px; max-width: 100%; display: block;">     

      See <a href="https://team-ocean.github.io/NFYB21003U-Fluid-Mechanics/Lecture8AOverview.html">Introduction to Dynamical Systems and Chaos</a> to read more about bifurcation, chaos, and fractals.
    <h2>Neural Networks for Dynamical Systems</h2>


    <h2>Neural Ordinary Differential Equations</h2>
    In a typical neural network the hidden state is represented as a series of discrete transformations:
\[
h(t+1) = f(h(t), \theta(t), t)
\]
where \(\theta\) is a parameterized hidden layer neural network:
\[
h(t+1) = \sigma (\mathbf{W}_t h(t) + b_t),
\]
but \(t\) can also be interpreted as continuous time. In this case, transitioning the input \(x\) at \(t=0\) to the output at \(t=N\) involves a sequence of transformations.<br>

However, one issue with this view is that it assumes the variable transitions are discrete, whereas many real-world systems, such as physical simulations, evolve continuously rather than through discrete updates. Thus a more reasonable choice is to model this problem as an Ordinary Differential Equation (ODE):
\[
\frac{d h(t)}{d t} = f(h(t), \theta(t), t)
\]
From another perspective, instead of directly modeling \(h(t)\), we might attempt to model its change rate \(\frac{d h(t)}{d t}\).<br>

Differential equations have been studied for centuries, and many numerical solvers exist to approximate their solutions. Instead of treating \(h(t)\) as a function to be optimized, we can view it as a process to be solved. Given an ODE and an initial state \(h(t_0)\), we can solve for the continuous-time function \(h(t)\) over any time interval \(t \in \mathbb{R}\).<br>

Assume that the final output state is \(h(t_1) = y\), the loss function can be defined in terms of the ODE solver:
\[
\mathcal{L} (h(t_1)) = \mathcal{L} (\text{ODESolve}(h(t_0), f, t_0, t_1, \theta))
\]
which can be rewritten as:
\[
\mathcal{L} \left(h(t_0) + \int_{t_0}^{t_1} f(h(t), t) dt \right)
\]





    <h3>Talk Is Cheap. Show Me The Code.</h3>




    <p><strong>Next</strong>: <a href="SailbyNightPhysics.html">Sail by Night Physics</a></p>

<div class="footer">
  Qi-fan will cheerfully accept full responsibility for any errors or misunderstandings which may
appear, despite so much assistance I received!<br>
  It is very much a work in progress! Have you spotted a mistake or an error on this page? 
  Click <a href="mailto:qifan.wu@nbi.ku.dk">here</a> to tell me!<br>
  &copy; 2025 TeamOcean | NBI/KU
</div>
</div>

<script>
  function copyToClipboard(btn) {
    const code = btn.nextElementSibling.innerText;
    navigator.clipboard.writeText(code).then(() => {
      btn.classList.add('copied');
      setTimeout(() => btn.classList.remove('copied'), 2000);
    });
  }
</script>
</body>
</html>